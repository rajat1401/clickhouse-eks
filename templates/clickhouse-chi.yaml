apiVersion: clickhouse.altinity.com/v1
kind: ClickHouseInstallation
metadata:
  name: clickhouse-cluster
  namespace: clickhouse
spec:
  configuration:
    zookeeper:
      nodes:
        {{- $keeperName := .Values.keeper.fullnameOverride | default "clickhouse-cluster-keeper" }}
        {{- $clusterName := .Values.keeper.clusterName | default "keeper-cluster" }}
        {{- $replicas := .Values.keeper.replicaCount | default 3 | int }}
        {{- $namespace := .Release.Namespace }}
        {{- range $i, $e := until $replicas }}
        - host: chk-{{ $keeperName }}-{{ $clusterName }}-0-{{ $i }}.{{ $namespace }}.svc.cluster.local
          port: 2181
        {{- end }}
    
    clusters:
      - name: cluster_1S_3R
        layout:
          shardsCount: 1
          replicasCount: 3
        templates:
          serviceTemplate: clickhouse-lb
    
    users:
      admin/networks/ip: 0.0.0.0/0
      admin/profile: default
      Admin/password: {{ .Values.adminPassword }}
      admin/access_management: "1"
      admin/settings/allow_ddl: 1
      admin/settings/readonly: 0

    files:
      config.d/logging.xml: |
        <clickhouse>
          <logger>
            <level>information</level>
            <log>/var/lib/clickhouse/clickhouse-server/clickhouse-server.log</log>
            <errorlog>/var/lib/clickhouse/clickhouse-server/clickhouse-server.err.log</errorlog>
            <size>100M</size>
            <count>5</count>
          </logger>
        </clickhouse>
      config.d/storage_configuration.xml: |
        <clickhouse>
          <storage_configuration>
            <disks>
              <default>
                <keep_free_space_bytes>10737418240</keep_free_space_bytes>
              </default>
              <s3_disk>
                  <type>s3</type>
                  <endpoint>https://s3.ap-south-1.amazonaws.com/gokwik-data-container/clickhouse-cold-eks/</endpoint>
                  <use_environment_credentials>true</use_environment_credentials>
                  <region>ap-south-1</region>
                  <metadata_path>/var/lib/clickhouse/disks/s3_disk/</metadata_path>
                  <skip_access_check>false</skip_access_check>
                  <send_metadata>true</send_metadata>
                  <support_batch_delete>true</support_batch_delete>
                  <objects_chunk_size_to_delete>1000</objects_chunk_size_to_delete>
                  <max_single_part_upload_size>33554432</max_single_part_upload_size> <!-- 32MB -->
                  <cache_enabled>true</cache_enabled>
                  <cache_path>/var/lib/clickhouse/disks/s3_cache/</cache_path>
                  <cache_size>50Gi</cache_size>
              </s3_disk>
            </disks>
            <policies>
              <default>
                <volumes>
                    <hot_volume>
                        <disk>default</disk>
                    </hot_volume>
                </volumes>
              </default>
              <s3_only>
                <volumes>
                  <main>
                    <disk>s3_disk</disk>
                  </main>
                </volumes>
              </s3_only>
              <tiered_storage>
                <volumes>
                  <hot_volume>
                    <disk>default</disk>
                  </hot_volume>
                  <cold_volume>
                    <disk>s3_disk</disk>
                    <max_data_part_size_bytes>1Gi</max_data_part_size_bytes>
                    <perform_ttl_move_on_insert>true</perform_ttl_move_on_insert>
                  </cold_volume>
                </volumes>
                <move_factor>0.1</move_factor>
                <prefer_not_to_merge>false</prefer_not_to_merge>
              </tiered_storage>
            </policies>
          </storage_configuration>
        </clickhouse>
  defaults:
    templates:
      podTemplate: clickhouse-pod-template
      dataVolumeClaimTemplate: data-volume
  templates:
    podTemplates:
      - name: clickhouse-pod-template
        spec:
          nodeSelector:
            role: clickhouse-data
          
          # Tolerate the taint we set on ClickHouse nodes
          tolerations:
            - key: workload
              operator: Equal
              value: clickhouse-data
              effect: NoSchedule # new pods that don't tolerate the taint won't be scheduled on that node
          
          # Anti-affinity: spread replicas across availability zones
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - clickhouse-data
                  topologyKey: topology.kubernetes.io/zone
          
          containers:
            - name: clickhouse
              image: clickhouse/clickhouse-server:25.11
              
              resources:
                requests:
                  memory: "160Gi"
                  cpu: "40"
                limits:
                  memory: "178Gi" #192gb max (- approx 10% for os,kubelet)
                  cpu: "46"
              
              # ClickHouse configuration for m8g.12xlarge
              env:
                - name: CLICKHOUSE_MAX_MEMORY_USAGE
                  value: "8000000000"  # 8GB max per query
                - name: CLICKHOUSE_MAX_CONCURRENT_QUERIES_FOR_USER
                  value: "20"   # Max concurrent queries per user
                - name: CLICKHOUSE_MAX_THREADS
                  value: "32"  # Parallel query execution
                - name: CLICKHOUSE_BACKGROUND_POOL_SIZE
                  value: "16"  # Background merges
                - name: CLICKHOUSE_BACKGROUND_SCHEDULE_POOL_SIZE
                  value: "16"  # Background scheduling tasks
              volumeMounts:
                - name: data-volume
                  mountPath: /var/lib/clickhouse
            - name: clickhouse-backup
              image: altinity/clickhouse-backup:master
              imagePullPolicy: IfNotPresent
              command:
                - "/bin/sh"
                - "-c"
                - |
                  while true; do
                    date
                    echo "Starting backup..."
                    clickhouse-backup create_remote $(date +%Y-%m-%d-%H-%M-%S)
                    echo "Backup finished."
                    echo "Cleaning old backups..."
                    clickhouse-backup delete local --older-than 6h
                    clickhouse-backup delete remote --older-than 24h
                    echo "Sleeping for 6 hours..."
                    sleep 21600
                  done
              env:
                - name: CLICKHOUSE_BACKUP_CONFIG_CLICKHOUSE_HOST
                  value: "127.0.0.1"
                - name: CLICKHOUSE_BACKUP_CONFIG_CLICKHOUSE_PORT
                  value: "8123"
                - name: CLICKHOUSE_BACKUP_CONFIG_CLICKHOUSE_USER
                  value: "admin"
                - name: CLICKHOUSE_BACKUP_CONFIG_CLICKHOUSE_PASSWORD
                  value: {{ .Values.adminPassword }}
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_BUCKET
                  value: {{ .Values.s3.bucket }}
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_REGION
                  value: {{ .Values.s3.region }}
                {{- if .Values.s3.accessKey }}
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_ACCESS_KEY
                  value: {{ .Values.s3.accessKey }}
                {{- end }}
                {{- if .Values.s3.secretKey }}
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_SECRET_KEY
                  value: {{ .Values.s3.secretKey }}
                {{- end }}
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_ENDPOINT
                  value: {{ .Values.s3.endpoint }}
                - name: CLICKHOUSE_BACKUP_CONFIG_GENERAL_REMOTE_STORAGE
                  value: "s3"
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_COMPRESSION_FORMAT
                  value: "gzip"
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_COMPRESSION_LEVEL
                  value: "1"
              volumeMounts:
                - name: data-volume
                  mountPath: /var/lib/clickhouse
    volumeClaimTemplates:
      - name: data-volume
        spec:
          storageClassName: clickhouse-storage
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 4Ti  # 4TB per node
    serviceTemplates:
      - name: clickhouse-lb
        metadata:
          annotations:
            service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
            service.beta.kubernetes.io/aws-load-balancer-internal: "true"
        spec:
          type: LoadBalancer
          ports:
            - name: http
              port: 8123
            - name: tcp
              port: 9000
---
apiVersion: clickhouse-keeper.altinity.com/v1
kind: ClickHouseKeeperInstallation
metadata:
  name: {{ .Values.keeper.fullnameOverride | default "clickhouse-cluster-keeper" }}
  namespace: clickhouse
spec:
  configuration:
    settings:
      logger/level: "information"
      logger/console: "1"
      listen_host: "0.0.0.0"
      
      # Enable Prometheus metrics
      prometheus/endpoint: "/metrics"
      prometheus/port: "7000"
      prometheus/metrics: "true"
      prometheus/events: "true"
      prometheus/asynchronous_metrics: "true"
      prometheus/status_info: "true"

      # Important for external monitoring tools
      keeper_server/four_letter_word_white_list: "*"
      
      # Raft coordination settings
      keeper_server/coordination_settings/raft_logs_level: "information"
      
      # HTTP Control
      keeper_server/http_control/port: "9182"
      keeper_server/http_control/readiness/endpoint: "/ready"

    clusters:
      - name: {{ .Values.keeper.clusterName | default "keeper-cluster" }}
        layout:
          replicasCount: {{ .Values.keeper.replicaCount | default 3 }}
  
  templates:
    podTemplates:
      - name: keeper-pod-template
        spec:
          nodeSelector:
            role: keeper
          
          # Anti-affinity: spread Keeper across availability zones
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - clickhouse-keeper
                  topologyKey: topology.kubernetes.io/zone
          
          containers:
            - name: clickhouse-keeper
              image: clickhouse/clickhouse-keeper:25.11
              
              resources:
                requests:
                  memory: "6Gi"   # m7g.xlarge has 16GB
                  cpu: "2"        # 4 vCPU available
                limits:
                  memory: "10Gi"
                  cpu: "4"
    
    volumeClaimTemplates:
      - name: keeper-storage
        spec:
          storageClassName: keeper-storage
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 50Gi