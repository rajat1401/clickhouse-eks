apiVersion: clickhouse.altinity.com/v1
kind: ClickHouseInstallation
metadata:
  name: clickhouse-cluster
  namespace: clickhouse
spec:
  configuration:
    zookeeper:
      nodes:
        {{- $keeperName := .Values.keeper.fullnameOverride | default "clickhouse-cluster-keeper" }}
        {{- $clusterName := .Values.keeper.clusterName | default "keeper-cluster" }}
        {{- $replicas := .Values.keeper.replicaCount | default 3 | int }}
        {{- $namespace := .Release.Namespace }}
        {{- range $i, $e := until $replicas }}
        - host: chk-{{ $keeperName }}-{{ $clusterName }}-0-{{ $i }}.{{ $namespace }}.svc.cluster.local
          port: 2181
        {{- end }}
    
    clusters:
      - name: primesuspect
        layout:
          shardsCount: 1
          replicasCount: 2
    
    users:
      admin/password: {{ .Values.adminPassword }}
      admin/networks/ip:
        - "0.0.0.0/0"
      admin/profile: default
      admin/access_management: "1"
      admin/settings/allow_ddl: 1
      admin/settings/readonly: 0

    files:
      config.d/logging.xml: |
        <clickhouse>
          <logger>
            <level>information</level>
            <log>/var/lib/clickhouse/clickhouse-server/clickhouse-server.log</log>
            <errorlog>/var/lib/clickhouse/clickhouse-server/clickhouse-server.err.log</errorlog>
            <size>100M</size>
            <count>5</count>
          </logger>
        </clickhouse>
      config.d/storage_configuration.xml: |
        <clickhouse>
          <storage_configuration>
            <disks>
              <default>
                <keep_free_space_bytes>104857600</keep_free_space_bytes>
              </default>
            </disks>
            <policies>
              <default>
                <volumes>
                    <hot_volume>
                        <disk>default</disk>
                    </hot_volume>
                </volumes>
              </default>
            </policies>
          </storage_configuration>
        </clickhouse>
  defaults:
    templates:
      podTemplate: clickhouse-pod-template
      dataVolumeClaimTemplate: data-volume
  templates:
    podTemplates:
      - name: clickhouse-pod-template
        spec:
          # nodeSelector:
          #   role: clickhouse-data
          
          # Tolerate the taint we set on ClickHouse nodes
          # tolerations:
          #   - key: workload
          #     operator: Equal
          #     value: clickhouse-data
          #     effect: NoSchedule # new pods that don't tolerate the taint won't be scheduled on that node
          
          # Anti-affinity: spread replicas across nodes
          # affinity:
          #   podAntiAffinity:
          #     requiredDuringSchedulingIgnoredDuringExecution:
          #       - labelSelector:
          #           matchExpressions:
          #             - key: app
          #               operator: In
          #               values:
          #                 - clickhouse-data
          #         topologyKey: kubernetes.io/hostname
          
          containers:
            - name: clickhouse
              image: clickhouse/clickhouse-server:25.11
              
              resources:
                requests:
                  memory: "2Gi"
                  cpu: "500m"
                limits:
                  memory: "3Gi"
                  cpu: "1"
              
              # ClickHouse configuration for m8g.12xlarge
              env:
                - name: CLICKHOUSE_MAX_MEMORY_USAGE
                  value: "3000000000"  # 3GB max per query
                - name: CLICKHOUSE_MAX_CONCURRENT_QUERIES_FOR_USER
                  value: "1"   # Max concurrent queries per user
                - name: CLICKHOUSE_MAX_THREADS
                  value: "1"  # Parallel query execution
                - name: CLICKHOUSE_BACKGROUND_POOL_SIZE
                  value: "1"  # Background merges
                - name: CLICKHOUSE_BACKGROUND_SCHEDULE_POOL_SIZE
                  value: "1"  # Background scheduling tasks
              volumeMounts:
                - name: data-volume
                  mountPath: /var/lib/clickhouse
                  
              livenessProbe:
                httpGet:
                  path: /ping
                  port: 8123
                initialDelaySeconds: 30
                periodSeconds: 10
            - name: clickhouse-backup
              image: altinity/clickhouse-backup:master
              imagePullPolicy: IfNotPresent
              command:
                - "/bin/sh"
                - "-c"
                - |
                  while true; do
                    date
                    echo "Starting backup..."
                    clickhouse-backup create_remote $(date +%Y-%m-%d-%H-%M-%S)
                    echo "Backup finished."
                    echo "Cleaning old backups..."
                    clickhouse-backup delete local --older-than 6h
                    clickhouse-backup delete remote --older-than 24h
                    echo "Sleeping for 6 hours..."
                    sleep 21600
                  done
              env:
                - name: CLICKHOUSE_BACKUP_CONFIG_CLICKHOUSE_HOST
                  value: "127.0.0.1"
                - name: CLICKHOUSE_BACKUP_CONFIG_CLICKHOUSE_PORT
                  value: "8123"
                - name: CLICKHOUSE_BACKUP_CONFIG_CLICKHOUSE_USER
                  value: "admin"
                - name: CLICKHOUSE_BACKUP_CONFIG_CLICKHOUSE_PASSWORD
                  value: {{ .Values.adminPassword }}
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_BUCKET
                  value: {{ .Values.s3.bucket }}
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_REGION
                  value: {{ .Values.s3.region }}
                {{- if .Values.s3.accessKey }}
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_ACCESS_KEY
                  value: {{ .Values.s3.accessKey }}
                {{- end }}
                {{- if .Values.s3.secretKey }}
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_SECRET_KEY
                  value: {{ .Values.s3.secretKey }}
                {{- end }}
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_ENDPOINT
                  value: {{ .Values.s3.endpoint }}
                - name: CLICKHOUSE_BACKUP_CONFIG_GENERAL_REMOTE_STORAGE
                  value: "s3"
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_COMPRESSION_FORMAT
                  value: "gzip"
                - name: CLICKHOUSE_BACKUP_CONFIG_S3_COMPRESSION_LEVEL
                  value: "1"
              volumeMounts:
                - name: data-volume
                  mountPath: /var/lib/clickhouse
    volumeClaimTemplates:
      - name: data-volume
        spec:
          storageClassName: clickhouse-storage
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 4Gi
---
apiVersion: clickhouse-keeper.altinity.com/v1
kind: ClickHouseKeeperInstallation
metadata:
  name: {{ .Values.keeper.fullnameOverride | default "clickhouse-cluster-keeper" }}
  namespace: clickhouse
spec:
  configuration:
    settings:
      logger/level: "information"
      logger/console: "1"
      listen_host: "0.0.0.0"
      
      # Enable Prometheus metrics
      prometheus/endpoint: "/metrics"
      prometheus/port: "7000"
      prometheus/metrics: "true"
      prometheus/events: "true"
      prometheus/asynchronous_metrics: "true"
      prometheus/status_info: "true"

      # Important for external monitoring tools
      keeper_server/four_letter_word_white_list: "*"
      
      # Raft coordination settings
      keeper_server/coordination_settings/raft_logs_level: "information"
      
      # HTTP Control
      keeper_server/http_control/port: "9182"
      keeper_server/http_control/readiness/endpoint: "/ready"

    clusters:
      - name: {{ .Values.keeper.clusterName | default "keeper-cluster" }}
        layout:
          replicasCount: {{ .Values.keeper.replicaCount | default 3 }}
  
  templates:
    podTemplates:
      - name: keeper-pod-template
        spec:
          # nodeSelector:
          #   role: keeper
          
          # Anti-affinity: spread Keeper across nodes
          # affinity:
          #   podAntiAffinity:
          #     requiredDuringSchedulingIgnoredDuringExecution:
          #       - labelSelector:
          #           matchExpressions:
          #             - key: app
          #               operator: In
          #               values:
          #                 - clickhouse-keeper
          #         topologyKey: kubernetes.io/hostname
          
          containers:
            - name: clickhouse-keeper
              image: clickhouse/clickhouse-keeper:25.11
              
              resources:
                requests:
                  memory: "1Gi"
                  cpu: "500m"
                limits:
                  memory: "1Gi"
                  cpu: "1"
    
    volumeClaimTemplates:
      - name: keeper-storage
        spec:
          storageClassName: keeper-storage
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 2Gi